seed: 9

total_timesteps: 2000000

run_notes: null

model_base_path: null  # Folder to load the model from
model_checkpoint: null  # Model checkpoint zip file name (without .zip) to load

compute:
  n_gpu_workers: 1
  n_cpu_workers: 8  # One gym env per cpu

logging:
  log_freq: 300  # How many times to log during training
  video_save_freq: 20000 # Frequency to save the video
  model_save_freq: 200000  # Frequency to save the model

  wandb_project: crossQ
  wandb_mode: online
  wandb_tags: []

  run_name: null
  run_path: null

success_eval:
  threshold_for_all_pos: 0.50
  threshold_for_arm_pos: 0.55

defaults:
  - env: HumanoidSpawnedUpCustom
  - visual_reward_model: nothing # hacky way to define no visual reward model 
  - matching_reward_model: nothing 
  - matching_reward_model/modification: nothing
  - rl_algo: sb3_sac
  - _self_

hydra:
  run:
    dir: ./train_logs/${now:%Y-%m-%d-%H%M%S}_${rl_algo.name}_envr=${env.reward_type}-t=${env.task_name}_mrm=${matching_reward_model.name}_vrm=${visual_reward_model.name}_nt=${run_notes}

# If you want to specify a specific reward function used in the env, you can uncomment and overwrite reward_type
# env:
#   reward_type: both_arms_out_goal_only_euclidean

# If you are using patch wasserstein as reward, you can uncomment and overwrite pos_image_path, neg_image_path in the dino_patch_wasserstein.yaml
# reward_model:
#   pos_image_path:
#     - axis_exp/humanoid_kneeling_ref.png
